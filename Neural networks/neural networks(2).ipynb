{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#starting time\n",
    "\n",
    "start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "# import matplotlib.pyplot as plt\n",
    "np.random.seed(32)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/sonu/Downloads/GrammarandProductReviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>dateAdded</th>\n",
       "      <th>dateUpdated</th>\n",
       "      <th>ean</th>\n",
       "      <th>keys</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>manufacturerNumber</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.id</th>\n",
       "      <th>reviews.numHelpful</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "      <th>upc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AV13O1A8GV-KLJ3akUyj</td>\n",
       "      <td>Universal Music</td>\n",
       "      <td>Movies, Music &amp; Books,Music,R&amp;b,Movies &amp; TV,Mo...</td>\n",
       "      <td>2017-07-25T00:52:42Z</td>\n",
       "      <td>2018-02-05T08:36:58Z</td>\n",
       "      <td>6.02537E+11</td>\n",
       "      <td>602537205981,universalmusic/14331328,universal...</td>\n",
       "      <td>Universal Music Group / Cash Money</td>\n",
       "      <td>14331328</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>https://redsky.target.com/groot-domain-api/v1/...</td>\n",
       "      <td>i love this album. it's very good. more to the...</td>\n",
       "      <td>Just Awesome</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joshua</td>\n",
       "      <td>6.02537E+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>2017-07-25T05:16:03Z</td>\n",
       "      <td>2018-02-05T11:27:45Z</td>\n",
       "      <td>73416000391</td>\n",
       "      <td>lundbergorganiccinnamontoastricecakes/b000fvzw...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>574764</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>...</td>\n",
       "      <td>100209113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.walmart.com/reviews/product/29775278</td>\n",
       "      <td>Good flavor. This review was collected as part...</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorothy W</td>\n",
       "      <td>73416000391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>2017-07-25T05:16:03Z</td>\n",
       "      <td>2018-02-05T11:27:45Z</td>\n",
       "      <td>73416000391</td>\n",
       "      <td>lundbergorganiccinnamontoastricecakes/b000fvzw...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>574764</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>...</td>\n",
       "      <td>100209113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.walmart.com/reviews/product/29775278</td>\n",
       "      <td>Good flavor.</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dorothy W</td>\n",
       "      <td>73416000391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>2017-07-25T16:26:19Z</td>\n",
       "      <td>2018-02-05T11:25:51Z</td>\n",
       "      <td>67981934427</td>\n",
       "      <td>kylovesensualitypleasuregel/b00u2whx8s,0679819...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>67981934427</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>...</td>\n",
       "      <td>113026909.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.walmart.com/reviews/product/43383370</td>\n",
       "      <td>I read through the reviews on here before look...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>67981934427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>2017-07-25T16:26:19Z</td>\n",
       "      <td>2018-02-05T11:25:51Z</td>\n",
       "      <td>67981934427</td>\n",
       "      <td>kylovesensualitypleasuregel/b00u2whx8s,0679819...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>67981934427</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>...</td>\n",
       "      <td>171267657.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.walmart.com/reviews/product/43383370</td>\n",
       "      <td>My husband bought this gel for us. The gel cau...</td>\n",
       "      <td>Irritation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Walker557</td>\n",
       "      <td>67981934427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id            brand  \\\n",
       "0  AV13O1A8GV-KLJ3akUyj  Universal Music   \n",
       "1  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "2  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "3  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "4  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "\n",
       "                                          categories             dateAdded  \\\n",
       "0  Movies, Music & Books,Music,R&b,Movies & TV,Mo...  2017-07-25T00:52:42Z   \n",
       "1  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...  2017-07-25T05:16:03Z   \n",
       "2  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...  2017-07-25T05:16:03Z   \n",
       "3  Personal Care,Medicine Cabinet,Lubricant/Sperm...  2017-07-25T16:26:19Z   \n",
       "4  Personal Care,Medicine Cabinet,Lubricant/Sperm...  2017-07-25T16:26:19Z   \n",
       "\n",
       "            dateUpdated          ean  \\\n",
       "0  2018-02-05T08:36:58Z  6.02537E+11   \n",
       "1  2018-02-05T11:27:45Z  73416000391   \n",
       "2  2018-02-05T11:27:45Z  73416000391   \n",
       "3  2018-02-05T11:25:51Z  67981934427   \n",
       "4  2018-02-05T11:25:51Z  67981934427   \n",
       "\n",
       "                                                keys  \\\n",
       "0  602537205981,universalmusic/14331328,universal...   \n",
       "1  lundbergorganiccinnamontoastricecakes/b000fvzw...   \n",
       "2  lundbergorganiccinnamontoastricecakes/b000fvzw...   \n",
       "3  kylovesensualitypleasuregel/b00u2whx8s,0679819...   \n",
       "4  kylovesensualitypleasuregel/b00u2whx8s,0679819...   \n",
       "\n",
       "                         manufacturer manufacturerNumber  \\\n",
       "0  Universal Music Group / Cash Money           14331328   \n",
       "1                            Lundberg             574764   \n",
       "2                            Lundberg             574764   \n",
       "3                                 K-Y        67981934427   \n",
       "4                                 K-Y        67981934427   \n",
       "\n",
       "                                         name  ...   reviews.id  \\\n",
       "0   Pink Friday: Roman Reloaded Re-Up (w/dvd)  ...          NaN   \n",
       "1  Lundberg Organic Cinnamon Toast Rice Cakes  ...  100209113.0   \n",
       "2  Lundberg Organic Cinnamon Toast Rice Cakes  ...  100209113.0   \n",
       "3            K-Y Love Sensuality Pleasure Gel  ...  113026909.0   \n",
       "4            K-Y Love Sensuality Pleasure Gel  ...  171267657.0   \n",
       "\n",
       "  reviews.numHelpful reviews.rating  \\\n",
       "0                0.0              5   \n",
       "1                NaN              5   \n",
       "2                NaN              5   \n",
       "3                NaN              1   \n",
       "4                NaN              1   \n",
       "\n",
       "                                  reviews.sourceURLs  \\\n",
       "0  https://redsky.target.com/groot-domain-api/v1/...   \n",
       "1   https://www.walmart.com/reviews/product/29775278   \n",
       "2   https://www.walmart.com/reviews/product/29775278   \n",
       "3   https://www.walmart.com/reviews/product/43383370   \n",
       "4   https://www.walmart.com/reviews/product/43383370   \n",
       "\n",
       "                                        reviews.text  reviews.title  \\\n",
       "0  i love this album. it's very good. more to the...   Just Awesome   \n",
       "1  Good flavor. This review was collected as part...           Good   \n",
       "2                                       Good flavor.           Good   \n",
       "3  I read through the reviews on here before look...   Disappointed   \n",
       "4  My husband bought this gel for us. The gel cau...     Irritation   \n",
       "\n",
       "   reviews.userCity  reviews.userProvince reviews.username          upc  \n",
       "0       Los Angeles                   NaN           Joshua  6.02537E+11  \n",
       "1               NaN                   NaN        Dorothy W  73416000391  \n",
       "2               NaN                   NaN        Dorothy W  73416000391  \n",
       "3               NaN                   NaN          Rebecca  67981934427  \n",
       "4               NaN                   NaN        Walker557  67981934427  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram \n",
    "# plt.hist(df['reviews.rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['reviews.rating']<4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, train_y, test_y = train_test_split(df['reviews.text'],df['target'],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56835,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonu/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26057 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# get the raw text data\n",
    "texts_train = train_text.astype(str)\n",
    "texts_test = test_text.astype(str)\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[589,\n",
       " 37,\n",
       " 1,\n",
       " 128,\n",
       " 80,\n",
       " 125,\n",
       " 5,\n",
       " 690,\n",
       " 29,\n",
       " 61,\n",
       " 3,\n",
       " 244,\n",
       " 130,\n",
       " 341,\n",
       " 17,\n",
       " 21,\n",
       " 1,\n",
       " 887,\n",
       " 8,\n",
       " 1286,\n",
       " 919,\n",
       " 58,\n",
       " 527,\n",
       " 200,\n",
       " 130,\n",
       " 273,\n",
       " 41,\n",
       " 1626,\n",
       " 804,\n",
       " 94,\n",
       " 117,\n",
       " 3,\n",
       " 19,\n",
       " 72,\n",
       " 612,\n",
       " 110,\n",
       " 44,\n",
       " 85,\n",
       " 180,\n",
       " 1082,\n",
       " 108,\n",
       " 29,\n",
       " 319,\n",
       " 112,\n",
       " 68,\n",
       " 402,\n",
       " 88,\n",
       " 489,\n",
       " 2,\n",
       " 44,\n",
       " 211,\n",
       " 30,\n",
       " 6,\n",
       " 18,\n",
       " 767]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 26057)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.word_index), len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i help in my kids classroom every time i help i always wipe the desks down with these wipes fast easy and man do they sterilize and clean the whole room smells clean after a good wiping i also do the mouse keyboards for the computers the teacher informed me that they don't have janitors and i am the only one cleaning the desks that makes these easy to use clorox wipes even more important to me\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([index_to_word[i] for i in sequences[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length: 39.4\n",
      "max length: 1034\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"average length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"max length: %d\" % max(seq_lens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(seq_lens, bins=50);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([l for l in seq_lens if l < 200], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (56835, 150)\n",
      "Shape of data test tensor: (14209, 150)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "# pad sequences with 0s\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (56835, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_y\n",
    "y_test = test_y\n",
    "\n",
    "y_train = to_categorical(np.asarray(y_train))\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "N_CLASSES = 2\n",
    "\n",
    "# input: a sequence of MAX_SEQUENCE_LENGTH integers\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "average = GlobalAveragePooling1D()(embedded_sequences)\n",
    "predictions = Dense(N_CLASSES, activation='softmax')(average)\n",
    "\n",
    "model = Model(sequence_input, predictions)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51151 samples, validate on 5684 samples\n",
      "Epoch 1/10\n",
      "51151/51151 [==============================] - 6s 116us/step - loss: 0.3999 - acc: 0.8616 - val_loss: 0.3597 - val_acc: 0.8628\n",
      "Epoch 2/10\n",
      "51151/51151 [==============================] - 7s 128us/step - loss: 0.3226 - acc: 0.8720 - val_loss: 0.3048 - val_acc: 0.8774\n",
      "Epoch 3/10\n",
      "51151/51151 [==============================] - 6s 116us/step - loss: 0.2699 - acc: 0.8900 - val_loss: 0.2695 - val_acc: 0.8890\n",
      "Epoch 4/10\n",
      "51151/51151 [==============================] - 6s 113us/step - loss: 0.2388 - acc: 0.9025 - val_loss: 0.2549 - val_acc: 0.9015\n",
      "Epoch 5/10\n",
      "51151/51151 [==============================] - 6s 114us/step - loss: 0.2202 - acc: 0.9121 - val_loss: 0.2440 - val_acc: 0.9039\n",
      "Epoch 6/10\n",
      "51151/51151 [==============================] - 6s 114us/step - loss: 0.2074 - acc: 0.9176 - val_loss: 0.2404 - val_acc: 0.9061\n",
      "Epoch 7/10\n",
      "51151/51151 [==============================] - 6s 116us/step - loss: 0.1972 - acc: 0.9232 - val_loss: 0.2360 - val_acc: 0.9092\n",
      "Epoch 8/10\n",
      "51151/51151 [==============================] - 6s 117us/step - loss: 0.1891 - acc: 0.9270 - val_loss: 0.2339 - val_acc: 0.9099\n",
      "Epoch 9/10\n",
      "51151/51151 [==============================] - 6s 116us/step - loss: 0.1818 - acc: 0.9303 - val_loss: 0.2330 - val_acc: 0.9119\n",
      "Epoch 10/10\n",
      "51151/51151 [==============================] - 6s 118us/step - loss: 0.1753 - acc: 0.9329 - val_loss: 0.2329 - val_acc: 0.9124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f089b49a828>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1,\n",
    "          nb_epoch=10, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.9111201207455728\n"
     ]
    }
   ],
   "source": [
    "output_test = model.predict(x_test)\n",
    "print(\"test auc:\", roc_auc_score(y_test,output_test[:,1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the program is 67.99579858779907\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "end=time.time()\n",
    "tot=end-start\n",
    "print(\"Runtime of the program is\",tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
